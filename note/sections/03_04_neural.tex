\subsection{Анализ с использованием нейронных сетей}
\label{sec:experiment:neural_engines}

Для сравнения также было решено разработать методику оценки стоимости недвижимости с использованием
нейронных сетей. Задача оценки недвижимости схематично представлена на рисунке~\ref{fig:experiment:neural-scheme}

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=0.8]{neural-scheme.jpg} 
  \caption{Cхема использования нейронных сетей для оценки стоимости недвижимости}
  \label{fig:experiment:neural-scheme}
\end{figure}

Для достижения цели необходимо выбрать факторы, влияющие на	рыночную стоимость объектов недвижимости, подготовить выборку для
обучения нейронной сети. Обучающая выборка построена для проектирования и обучения нейронной сети с учителем,
поскольку такой тип нейронных сетей больше всего подходит для задач, когда имеется большой набор настоящих данных для обучения
алгоритма. Исходя из сравнительного анализа нескольких типов нейронных сетей с учителем, проведенного в статье, было
решено использовать нейронную сеть многослойный персептрон c использованием метода обратного распространения ошибки.
Многослойным персептроном называют нейронную сеть прямого распространения, где входной сигнал распространяется от слоя
к слою в прямом направлении. В общем представлении такая нейронная сеть состоит из:
\begin{itemize}
  \item множества входных узлов, образующих входной слой;
  \item одного или нескольких скрытых слоев вычислительных нейронов;
  \item одного выходного слоя нейронов.
\end{itemize}

Обощенная схема многослойного персептрона показана на рисунке~\ref{fig:experiment:perceptron-scheme}
\begin{figure}[!ht]
  \centering
  \includegraphics[scale=2]{perceptron.png}
  \caption{Cхема многослойного персептрона}
  \label{fig:experiment:perceptron-scheme}
\end{figure}

В качестве инструментального средства проектирования нейронной сети была выбрана STATISTICA Neural Networks.
Для обучения многослойных персептронов в пакете STATISTICA Нейронные сети реализовано пять различных алгоритмов
обучения. Это хорошо известный алгоритм обратного распространения, быстрые методы второго порядка – спуск по сопряженным
градиентам и Левенберга–Маркара, а также методы быстрого распространения и «дельта–дельта с чертой» (представляющие
собой вариации метода обратного распространения, которые в некоторых случаях работают быстрее).

Алгоритм обратного распространения ошибки является популярным алгоритмом обучения нейронных сетей с учителем. В основе
идеи алгоритма лежит использование выходной ошибки нейронной сети для вычисления величин коррекции весов нейронов в скрытых слоях

\begin{equation}
  E = \frac{1}{2}\sum_{i=1}^{k}\left(y-y'\right)^2
\end{equation}

\begin{explanation}
  где & $ k $ & число выходных нейронов сети,\\
  & $ y $ & целевое значение, \\
  & $ y' $ & фактическое выходное значение.\\ 
\end{explanation}

Алгоритм является итеративным. На каждой итерации происходит прямой и обратный проходы. На прямом проходе входной вектор распространяется
входов сети к ее выходам, в результате формируется выходной вектор, который соответствует фактическому состоянию весов.
После вычисляется ошибка нейронной сети как разность между фактическим и целевым значениями.
На обратном проходе эта ошибка распространяется от выхода сети к ее входам, и производится коррекция весов нейронов.
Полученные данные дают возможность с достаточной точностью прогнозировать стоимость объектов недвижимости по заданным параметрам.

Для обучения нейронных сетей были также выбраны 3 модели с различными параметрами в качетсве входных данных. Сравнительная
характеристика полученных нейронных сетей показана в таблице~\ref{table:experiment:neural:comparing}

\begin{table}[!ht]
  \caption{Сравнительная характеристика построенных моделей}
  \label{table:experiment:neural:comparing}
  \centering
    \begin{tabular}{{ 
    |>{\centering}m{0.35\textwidth}
    |  m{0.17\textwidth}
    |  m{0.17\textwidth}
    |>{\raggedright\arraybackslash}m{0.17\textwidth}|}}
  
      \hline
      Модель & {\begin{center} Training performance \end{center}} & {\begin{center} Test performance \end{center}} & {\begin{center} Validation performance \end{center}} \\
  
      \hline
      Множественная модель & {\begin{center}0.91\end{center}} & {\begin{center}0.91\end{center}} & {\begin{center}0.91\end{center}}\\
      
      \hline
      Отдельные модели по числу комнат & {\begin{center}0.91\end{center}} & {\begin{center}0.91\end{center}} & {\begin{center}0.87\end{center}}\\
  
      \hline
      Отдельные модели по району & {\begin{center}0.92\end{center}} & {\begin{center}0.95\end{center}} & {\begin{center}0.93\end{center}}\\
  
    \hline
    \end{tabular}
  \end{table}

Исходя из сравнения полученных нейронных сетей можно сделать вывод, что наилучшие результаты показывают отдельные
модели по району. Однако результаты варьируются от района к району в зависимости от количества продаваемых объектов
в данном районе, поэтому для отдельных районов целесообразнее использовать модель по районам, для остальных районов - 
общую модель.

Реализованные алгоритмы обучения представлены на рисунке~\ref{fig:experiment:network_configs}.\linebreak Кроме того, можно
заметить, что в зависимости от алгоритма обучения и выбранной функции активации изменялась и конфигрурация многослойного
персептрона. Так, самой эффективной оказалась конфигурация с 8 нейроном на первом скрытом слое, на втором – 5. На
обучение отводилось 4000 объектов из обучающей выборки, что составляет 70\% от общего числа объектов,
на контроль и тестирование по 15\% - по 600 объектов.

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=1.2]{network_configs.png}
  \caption{Параметры качества реализованных алгоритмов обучения}
  \label{fig:experiment:network_configs}
\end{figure}

Сравнение результатов ожидаемых с полученными представлено на рисунке~\ref{fig:experiment:neural_results}.
В первом столбце указана ожидаемая стоимость, во втором - полученная, в третьем - величина ошибки.
Как можно заметить ошибка варьируется и может составлять как и небольшие значения так и значительные.
Соотношение ожидаемых результатов с полученными показаны в виде графика на рисунке~\ref{fig:experiment:target_vs_output_neural}.

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=1]{neural_results.png}
  \caption{Сравнение полученных результатов с ожидаемыми}
  \label{fig:experiment:neural_results}
\end{figure}

\begin{sidewaysfigure}
  \centering
  \includegraphics[scale=0.7]{target_vs_output_neural.png}
  \caption{Соотношение полученных результатов с ожидаемыми}
  \label{fig:experiment:target_vs_output_neural}
\end{sidewaysfigure}


