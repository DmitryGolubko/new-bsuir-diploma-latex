\subsection{Нейросетевой анализ}
\label{sec:analysis:neural_networks}

Нейронная сеть (также искусственная нейронная сеть, ИНС) — математическая модель, а также её программное или аппаратное
воплощение, построенная по принципу организации и функционирования биологических нейронных сетей — сетей нервных клеток
живого организма. Это понятие возникло при изучении процессов, протекающих в мозге, и при попытке смоделировать эти
процессы.

Нейронные сети используются для решения сложных задач, которые требуют аналитических вычислений подобных тем, что
делает человеческий мозг. Самыми распространенными применениями нейронных сетей является:

\begin{itemize}
  \item Классификация — распределение данных по параметрам. Например, на вход дается набор людей и нужно решить, кому из них давать кредит, а кому нет. Эту работу может сделать нейронная сеть, анализируя такую информацию как: возраст, платежеспособность, кредитная история и тд.
  \item Предсказание — возможность предсказывать следующий шаг. Например, рост или падение акций, основываясь на ситуации на фондовом рынке.
  \item Распознавание — в настоящее время, самое широкое применение нейронных сетей. Используется в Google, когда вы ищете фото или в камерах телефонов, когда оно определяет положение вашего лица и выделяет его и многое другое.
\end{itemize}

Классификация искусственных нейронных сетей представлена на рисунке~\ref{fig:analysis:neural-classification}

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=0.6]{neural-classification.jpg} 
  \caption{Классификация нейронных сетей}
  \label{fig:analysis:neural-classification}
\end{figure}

Задачи прогнозирования (предсказания) можно разбить на два основных класса: классификация и регрессия.

В задачах классификации нужно бывает определить, к какому из \linebreak
нескольких заданных классов принадлежит данный входной
набор. Примерами могут служить предоставление кредита (относится ли данное лицо к группе высокого или низкого
кредитного риска), диагностика раковых заболеваний (опухоль, чисто), распознавание подписи (поддельная, подлинная).
Во всех этих случаях, очевидно, на выходе требуется всего одна номинальная переменная.
Чаще всего (как в этих примерах) задачи классификации бывают двузначными, хотя встречаются и задачи с несколькими
возможными состояниями.

В задачах регрессии требуется предсказать значение переменной, принимающей (как правило) непрерывные числовые
значения: завтрашнюю цену акций, расход топлива в автомобиле, прибыли в следующем году и т.п.. В таких случаях в
качестве выходной требуется одна числовая переменная.

Исскусственные нейронные сети обычно состоят из нейронов и связывающих их синапсов.

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=1]{neurons.png} 
  \caption{Виды нейронов}
  \label{fig:analysis:neurons}
\end{figure}

Нейрон — это вычислительная единица, которая получает информацию, производит над ней простые вычисления и передает ее
дальше. Они делятся на три основных типа: входной (слева), скрытый (посередине) и выходной (справа). Также есть нейрон
смещения и контекстный нейрон. В том случае, когда нейросеть состоит из большого количества нейронов, вводят термин
слоя. Соответственно, есть входной слой, который получает информацию, \emph{n} скрытых слоев (обычно их не больше 3),
которые ее обрабатывают и выходной слой, который выводит результат. У каждого из нейронов есть 2 основных параметра:
входные данные (input data) и выходные данные (output data). В случае входного нейрона: input=output. В остальных,
в поле input попадает суммарная информация всех нейронов с предыдущего слоя, после чего, она нормализуется, с помощью
функции активации и попадает в поле output~\cite{neural-networks}.

Нейрон смещения или bias нейрон — это третий вид нейронов, используемый в большинстве нейросетей. Особенность этого
типа нейронов заключается в том, что его вход и выход всегда равняются 1 и они никогда не имеют входных синапсов.
Нейроны смещения могут, либо присутствовать в нейронной сети по одному на слое, либо полностью отсутствовать,
50/50 быть не может (красным на схеме обозначены веса и нейроны которые размещать нельзя). Соединения у нейронов
смещения такие же, как у обычных нейронов — со всеми нейронами следующего уровня, за исключением того, что синапсов
между двумя bias нейронами быть не может. Следовательно, их можно размещать на входном слое и всех скрытых слоях,
но никак не на выходном слое, так как им попросту не с чем будет формировать связь.

Нейрон смещения нужен для того, чтобы иметь возможность получать выходной результат, путем сдвига графика функции
активации вправо или влево.

Также нейроны смещения помогают в том случае, когда все входные нейроны получают на вход 0 и независимо от того какие
у них веса, они все передадут на следующий слой 0, но не в случае присутствия нейрона смещения.

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=3]{bias-neurons.png}
  \caption{Нейрон смещения}
  \label{fig:analysis:bias-neurons}
\end{figure}

Синапс -- это связь между двумя нейронами. У синапсов есть 1 параметр — вес. Благодаря ему, входная информация изменяется,
когда передается от одного нейрона к другому. Допустим, есть 3 нейрона, которые передают информацию следующему.
Тогда у нас есть 3 веса, соответствующие каждому из этих нейронов. У того нейрона, у которого вес будет больше, та
информация и будет доминирующей в следующем нейроне (пример — смешение цветов). На самом деле, совокупность весов
нейронной сети или матрица весов — это своеобразный мозг всей системы. Именно благодаря этим весам, входная информация
обрабатывается и превращается в результат.

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=3]{synaps.png}
  \caption{Синапсы для связи нейронов}
  \label{fig:analysis:synaps}
\end{figure}

Функция активации -- это способ нормализации входных данных. То есть, если на входе у
вас будет большое число, пропустив его через функцию активации, вы получите выход в нужном вам диапазоне. Самые
основные функции активации: Линейная, Сигмоид (Логистическая) и Гиперболический
тангенс. Главные их отличия — это диапазон значений.

Линейная функция почти никогда не используется, за исключением случаев, когда нужно протестировать нейронную сеть или
передать значение без преобразований.

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=0.5]{linear-activation.jpg} 
  \caption{Линейная функция активации}
  \label{fig:analysis:linear-activation}
\end{figure}

Сигмоид -- это самая распространенная функция активации, ее диапазон значений [0,1]. Именно на ней показано большинство примеров
в сети, также ее иногда называют логистической функцией. Соответственно, если в вашем случае присутствуют отрицательные
значения (например, акции могут идти не только вверх, но и вниз), то вам понадобиться функция которая захватывает и
отрицательные значения.

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=0.5]{sigmod-activation.jpg} 
  \caption{Сигмоид}
  \label{fig:analysis:sigmod-activation}
\end{figure}

Имеет смысл использовать гиперболический тангенс, только тогда, когда ваши значения могут быть и отрицательными, и
положительными, так как диапазон функции [-1,1]. Использовать эту функцию только с положительными значениями
нецелесообразно так как это значительно ухудшит результаты вашей нейросети.

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=0.5]{tangens-activation.jpg} 
  \caption{Гиперболический тангенс}
  \label{fig:analysis:tangens-activation}
\end{figure}

Ошибка — это процентная величина, отражающая расхождение между ожидаемым и полученным ответами. Ошибка формируется
каждую эпоху и должна идти на спад. Ошибку можно вычислить разными путями, например:
Mean Squared Error (далее MSE), Root MSE и Arctan.
Здесь нет какого-либо ограничения на использование, как в функции активации, и можно использовать любой метод, который
будет приносить вам наилучший результат. Стоит лишь учитывать, что каждый метод считает ошибки по разному.
У Arctan, ошибка, почти всегда, будет больше, так как он работает по принципу: чем больше разница, тем больше ошибка.
У Root MSE будет наименьшая ошибка, поэтому, чаще всего, используют MSE, которая сохраняет баланс в вычислении ошибки.

Обучение искусственных нейронных сетей происходит с помощью заранее подготовленной выборки данных (тренировочный сет).

Общее количество тренировочных сетов, пройденных нейронной сетью, называется итерацией.

Эпохой называется количество пройденных наборов тренировочных \linebreak сетов. Чем больше эпоха, тем лучше натренирована сеть и
соответственно, ее результат.

Для получения корректных результатов работы искусственной нейронной сети, её необходимо обучить. Существует несколько
методов обучения нейронных сетей, самые распространенные из них: метод обратного распространения,
метод упругого распространения, генетический алгоритм.

Метод обратного распространения, использует алгоритм градиентного спуска.

Применение алгоритма обратного распространения ошибки — один из известных методов, используемых для глубокого обучения
нейронных сетей прямого распространения (такие сети ещё называют многослойными персептронами). Этот метод относят к
методу обучения с учителем, поэтому требуется задавать в обучающих примерах целевые значения.

Сегодня нейронные сети прямого распространения используются для решения множества сложных задач. Если говорить об
обучении нейронных сетей методом обратного распространения, то тут пользуются двумя проходами по всем слоям нейросети:
прямым и обратным. При выполнении прямого прохода осуществляется подача входного вектора на входной слой сети, после
чего происходит распространение по нейронной сети от слоя к слою. В итоге должна осуществляться генерация набора
выходных сигналов — именно он, по сути, является реакцией нейронной сети на этот входной образ. При прямом проходе все
синаптические веса нейросети фиксированы. При обратном проходе все синаптические веса настраиваются согласно правил
коррекции ошибок, когда фактический выход нейронной сети вычитается из желаемого, что приводит к формированию сигнала
ошибки. Такой сигнал в дальнейшем распространяется по сети, причём направление распространения обратно направлению
синаптических связей. Именно поэтому соответствующий метод и называют алгоритмом с обратно распространённой ошибкой.
Синаптические веса настраивают с целью наибольшего приближения выходного сигнала нейронной сети к желаемому.

Цель обучения нейросети при использовании алгоритма обратного распространения ошибки — это такая подстройка весов
нейросети, которая позволит при приложении некоторого множества входов получить требуемое множество выходов нейронов
(выходных нейронов). Можно назвать эти множества входов и выходов векторами. В процессе обучения предполагается,
что для любого входного вектора существует целевой вектор, парный входному и задающий требуемый выход. Эту пару
называют обучающей. Работая с нейросетями, мы обучаем их на многих парах.

Также можно сказать, что алгоритм использует стохастический градиентный спуск и продвигается в многомерном пространстве
весов в направлении антиградиента, причём цель — это достижение минимума функции ошибки.

При практическом применении метода обучение продолжают не до \linebreak максимально точной настройки нейросети на минимум функции
ошибки, а пока не будет достигнуто довольно точное его приближение. С одной стороны, это даёт возможность уменьшить
количество итераций обучения, с другой — избежать переобучения нейронной сети.

Для реализации метода обратного распространения ошибки необходимо выполнить следующие действия:
\begin{enumerate}
  \item Инициализировать синаптические веса случайными маленькими значениями.
  \item Выбрать из обучающего множества очередную обучающую пару; подать на вход сети входной вектор.
  \item Выполнить вычисление выходных значений нейронной сети.
  \item Посчитать разность между выходом нейросети и требуемым выходом (речь идёт о целевом векторе обучающей пары).
  \item Скорректировать веса сети в целях минимизации ошибки.
  \item Повторять для каждого вектора обучающего множества шаги б-д, пока ошибка обучения нейронной сети на всём множестве не достигнет уровня, который является приемлемым.
\end{enumerate}

Сегодня существует много модификаций алгоритма обратного распространения ошибки. Возможно обучение не «по шагам»
(выходная ошибка вычисляется, веса корректируются на каждом примере), а «по эпохам» в offline-режиме (изменения весовых
коэффициентов происходит после подачи на \linebreak вход нейросети всех примеров обучающего множества, а ошибка обучения нейронной
сети усредняется по всем примерам).

Обучение «по эпохам» более устойчиво к выбросам и аномальным значениям целевой переменной благодаря усреднению ошибки
по многим примерам. Зато в данном случае увеличивается вероятность «застревания» в локальных минимумах. При обучении
«по шагам» такая вероятность меньше, ведь применение отдельных примеров создаёт «шум», «выталкивающий» алгоритм
обратного распространения из ям градиентного рельефа.

К плюсам можно отнести простоту в реализации и устойчивость к выбросам и аномалиям в данных, и это основные преимущества.
Но есть и минусы:
\begin{itemize}
  \item неопределенно долгий процесс обучения;
  \item вероятность «паралича сети» (при больших значениях рабочая точка функции активации попадает в область насыщения сигмоиды, а производная величина приближается к 0, в результате чего коррекции весов почти не происходят, а процесс обучения «замирает»;
  \item алгоритм уязвим к попаданию в локальные минимумы функции \linebreak ошибки.
\end{itemize}

Появление алгоритма стало знаковым событием и положительно отразилось на развитии нейросетей, ведь он реализует
эффективный с точки зрения вычислительных процессов способ обучения многослойного персептрона. В то же самое время,
было бы неправильным сказать, что алгоритм предлагает наиболее оптимальное решение всех потенциальных
проблем~\cite{Back_propagation_algorithm}.


Одним из серьезных недостатков алгоритма с обратным распространением ошибки, используемого для обучения многослойных
нейронных сетей, является слишком долгий процесс обучения, что делает неприменимым использование данного алгоритма для
широкого круга задач, которые требуют быстрого решения. В настоящее время известно достаточное количество алгоритмов
ускоряющих процесс обучения, одним из них является метод упругого распространения ошибки, который был
предложен М. Ридмиллером (M.Riedmiller) и Г. Брауном (H.Braun).

Нейросеть можно обучать с учителем и без.

Обучение с учителем — это тип тренировок присущий таким проблемам как регрессия и классификация. Иными словами здесь
вы выступаете в роли учителя, а НС в роли ученика. Вы предоставляете входные данные и желаемый результат, то есть
ученик посмотрев на входные данные поймет, что нужно стремиться к тому результату который ему предоставлен.

Обучение без учителя — этот тип обучения встречается не так часто. Здесь нет учителя, поэтому сеть не получает
желаемый результат или же их количество очень мало. В основном такой вид тренировок присущ НС у которых задача
состоит в группировке данных по определенным параметрам.

Существует еще такой метод, как обучение с подкреплением. Такой способ применим тогда, когда можно основываясь на
результатах полученных от НС, дать ей оценку. НС поредоставляется право найти любой способ достижения цели, до тех пор
пока он будет давать хороший результат. Таким способом, сеть начнет понимать чего от нее хотят добиться и пытается
найти наилучший способ достижения этой цели без постоянного предоставления данных “учителем”.
